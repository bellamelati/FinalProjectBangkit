# -*- coding: utf-8 -*-
"""SajiinDong.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18UvuYHZV2JGfl4TCU886exLv4pL8Q9_W

## **Aplikasi Rekomendasi Makanan**

##**Importing Libraries and Loading Data**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout

# Ideal and Target Weight Calculation Function
def calculate_target_weight(height_cm, weight_kg, goal_type, desired_mass_kg=0):
    """Calculate target weight based on user goals: 'diet' or 'bulking'."""
    if goal_type == 'diet':
        target_bmi = 22  # Assumed ideal BMI for diet
        target_weight = (height_cm / 100) ** 2 * target_bmi
    elif goal_type == 'bulking':
        target_weight = weight_kg + desired_mass_kg
    return target_weight

# Recommendation Function Based on Target Body Weight
def recommend_based_on_target_weight(data, target_weight, current_weight):
    """Recommend food items based on the target weight calculated."""
    if target_weight > current_weight:
        recommended = data[data['Caloric Density'] >= data['Caloric Density'].median()]
    else:
        recommended = data[data['Caloric Density'] <= data['Caloric Density'].median()]
    return recommended

# Load the datasets
food_df = pd.read_csv('Food.csv')
beverage_df = pd.read_csv('Beverage.csv')

# Concatenate both datasets
data = pd.concat([food_df, beverage_df], ignore_index=True)

"""### **Cleaning Data**"""

data.drop_duplicates(inplace=True)
data.fillna(method='ffill', inplace=True)

"""##**EDA**"""

# Descriptive statistics
print(data.describe())

# Visualization of categorical data: Category distribution
plt.figure(figsize=(10, 6))
sns.countplot(x='Category', data=data)
plt.title('Distribution of Categories')
plt.xlabel('Category')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# Visualization of numerical data: Histograms for caloric density
data['Caloric Density'] = data['Calories (kcal)'] / (data['Protein (g)'] + data['Carbohydrates (g)'] + data['Fats (g)'])
plt.figure(figsize=(10, 6))
sns.histplot(data['Caloric Density'], kde=True)
plt.title('Distribution of Caloric Density')
plt.xlabel('Caloric Density')
plt.ylabel('Frequency')
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(data[['Calories (kcal)', 'Protein (g)', 'Carbohydrates (g)', 'Fats (g)', 'Caloric Density']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

"""##**Feature Engineering**"""

# Feature Engineering: Adding a feature for caloric density
data['Caloric Density'] = data['Calories (kcal)'] / (data['Protein (g)'] + data['Carbohydrates (g)'] + data['Fats (g)'])

"""##**Preprocessing Data for Neural Networks**"""

# Assuming 'Category' column exists and has 'Diet' and 'Bulking' as values
numeric_features = ['Caloric Density', 'Protein (g)', 'Carbohydrates (g)', 'Fats (g)']
categorical_features = ['Category']  # Add if there are any categorical features to include

# Create a transformer Pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Prepare the features
X = preprocessor.fit_transform(data)
y = pd.get_dummies(data['Category']).values  # This assumes the target is categorical and named 'Category'

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""##**Modelling Deep Learning with Sequential Model Keras**"""

# Define the model
model = Sequential([
    Dense(128, activation='relu', input_dim=X_train.shape[1]),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(y_train.shape[1], activation='softmax')  # Use softmax for multi-class classification
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

"""###**Plotting Function**"""

# Plotting function
def plot_training_history(history):
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='train accuracy')
    plt.plot(history.history['val_accuracy'], label='validation accuracy')
    plt.title('Accuracy over epochs')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='train loss')
    plt.plot(history.history['val_loss'], label='validation loss')
    plt.title('Loss over epochs')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()

    plt.show()

plot_training_history(history)